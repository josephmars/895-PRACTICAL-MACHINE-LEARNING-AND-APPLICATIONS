{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 895 Practical Machine Learning and Applications - Project 1\n",
    "# Joseph Mart√≠nez Salcedo - 01236703"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in the folder: 18632\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def count_files_in_folder(folder_path):\n",
    "    folder = Path(folder_path)\n",
    "    return len([f for f in folder.iterdir() if f.is_file()])\n",
    "\n",
    "folder_path = './kaggle/train_images/'\n",
    "file_count = count_files_in_folder(folder_path)\n",
    "print(\"Number of files in the folder:\", file_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images = 18632\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>800113bb65efe69e.jpg</td>\n",
       "      <td>healthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8002cb321f8bfcdf.jpg</td>\n",
       "      <td>scab frog_eye_leaf_spot complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80070f7fb5e2ccaa.jpg</td>\n",
       "      <td>scab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80077517781fb94f.jpg</td>\n",
       "      <td>scab</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  image                           labels\n",
       "0  800113bb65efe69e.jpg                          healthy\n",
       "1  8002cb321f8bfcdf.jpg  scab frog_eye_leaf_spot complex\n",
       "2  80070f7fb5e2ccaa.jpg                             scab\n",
       "3  80077517781fb94f.jpg                             scab"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the images and labels from kaggle\n",
    "\n",
    "train_labels_path = './kaggle/train.csv'\n",
    "train_dir ='./kaggle/train_images'\n",
    "test_dir='./kaggle/test_images'\n",
    "\n",
    "train_labels=pd.read_csv(train_labels_path)\n",
    "\n",
    "\n",
    "print(f'Number of images = {len(train_labels)}')\n",
    "\n",
    "train_labels.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scab',\n",
       " 'healthy',\n",
       " 'frog_eye_leaf_spot',\n",
       " 'rust',\n",
       " 'complex',\n",
       " 'powdery_mildew',\n",
       " 'scab frog_eye_leaf_spot',\n",
       " 'scab frog_eye_leaf_spot complex',\n",
       " 'frog_eye_leaf_spot complex',\n",
       " 'rust frog_eye_leaf_spot',\n",
       " 'rust complex',\n",
       " 'powdery_mildew complex']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_list = list(train_labels[\"labels\"].value_counts().index)\n",
    "class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'healthy')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plotting one image\n",
    "\n",
    "i=0\n",
    "\n",
    "name_img = train_labels.iloc[i, 0]\n",
    "label_name_img=train_labels.iloc[i,1]\n",
    "\n",
    "img = Image.open(os.path.join(train_dir, name_img[:-4]+\".jpg\"))\n",
    "plt.imshow(img)\n",
    "plt.title(label_name_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizing the images to 64x64\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor,InterpolationMode, ColorJitter\n",
    "\n",
    "# Define data transformations\n",
    "resizing = Resize([64, 64],interpolation=InterpolationMode.BICUBIC)   #1. Resize the image to 64x64 pixels\n",
    "increase_saturation = ColorJitter(saturation=4)                         #2. Increase the saturation of the image\n",
    "\n",
    "\n",
    "transform = Compose([resizing,increase_saturation,ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(transform(img).mul(255).permute(1, 2, 0).byte().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomTextLabelDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_dir, transform=None):\n",
    "        self.csv_path = csv_path\n",
    "        self.images_root = image_dir\n",
    "        self.transform = transform\n",
    "        self.data = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        # Load data from the CSV file\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "\n",
    "        # Combine filename and label information\n",
    "        data = [{'filename': row['image'], 'label': row['labels']} for _, row in df.iterrows()]\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        entry = self.data[index]\n",
    "        image_path = os.path.join(self.images_root, entry['filename'])\n",
    "        text_label = entry['label']\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, text_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'healthy'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the path to CSV file and images directory\n",
    "\n",
    "## dataset:\n",
    "full_dataset = CustomTextLabelDataset(csv_path=train_labels_path, image_dir=train_dir, transform=transform)\n",
    "#access label\n",
    "full_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_model(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu3): ReLU()\n",
       "  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc): Linear(in_features=4096, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN_model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN_model, self).__init__()\n",
    "\n",
    "        #### 1st Convolutional Layer\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        # ReLU activation\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # Max-pooling layer\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #### 2nd Convolutional Layer\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        # ReLU activation\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # Max-pooling layer\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        #### 3rd Convolutional Layer\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        # ReLU activation\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # Max-pooling layer\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        ### Fully Connected Layer\n",
    "        self.fc = nn.Linear(64 * 32 * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool1(self.relu1(self.conv1(x)))\n",
    "        x = self.maxpool2(self.relu2(self.conv2(x)))\n",
    "        x = self.maxpool3(self.relu3(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "num_classes = 12  # we have 12 classes here\n",
    "model = CNN_model(num_classes)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        \n",
    "        #focul loss \n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha*(1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        #if self.alpha is not None:\n",
    "            #alpha_factor = self.alpha[targets]\n",
    "            #focal_loss = focal_loss * alpha_factor\n",
    "\n",
    "\n",
    "        # Apply reduction\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        elif self.reduction == 'none':\n",
    "            return focal_loss\n",
    "        else:\n",
    "            raise ValueError(\"Invalid reduction option. Use 'mean', 'sum', or 'none'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the labels are given in text format I made a mapping to turn text into numerics and vice-versa\n",
    "*********************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping between text labels and numerical indices\n",
    "unique_text_labels = class_list\n",
    "label_to_index = {label: index for index, label in enumerate(unique_text_labels)}\n",
    "index_to_label = {index: label for label, index in label_to_index.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING ON train_images:\n",
    "*************************************************************************************************\n",
    "Used Adam optimizer with learning rate 0.01.\n",
    "\n",
    "\n",
    "the model is saved locally at 'cnn_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1490\n",
      "1\n",
      "Epoch 1/4, Train Set Loss: 2.0683, Validation Set Accuracy: 0.2823\n",
      "Epoch 2/4, Train Set Loss: 1.8902, Validation Set Accuracy: 0.2715\n",
      "Epoch 3/4, Train Set Loss: 1.6784, Validation Set Accuracy: 0.2688\n",
      "Epoch 4/4, Train Set Loss: 1.4862, Validation Set Accuracy: 0.2581\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "#from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Define the focal loss function and optimizer:\n",
    "criterion = FocalLoss(gamma=2, alpha=1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#step_scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "# Wrap model and optimizer with XLA\n",
    "#model, optimizer = xm.initialize_model(model, optimizer, opt_level=\"O2\")\n",
    "\n",
    "\n",
    "# Split the dataset into training and validation sets: 80:20\n",
    "train_size = int(0.08 * len(full_dataset))\n",
    "val_size = int(0.02 * len(full_dataset))\n",
    "not_used_size = len(full_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, not_used = random_split(full_dataset, [train_size, val_size, not_used_size])\n",
    "print(len(train_dataset))\n",
    "\n",
    "# Create data loaders for training and validation\n",
    "batch_size = 2048\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(len(train_loader))\n",
    "\n",
    "\n",
    "# Use GPU for model\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    ## Training loop:\n",
    "    \n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    #run=0\n",
    "    for inputs, text_labels in train_loader:\n",
    "        inputs, text_labels = inputs.to(device), text_labels\n",
    "        \n",
    "        #print('input',inputs)\n",
    "        #print('text_labels',text_labels)\n",
    "        #print(inputs.shape)\n",
    "        \n",
    "        # Convert text labels to numerical indices using the mapping\n",
    "        numerical_labels = [label_to_index[label] for label in text_labels]\n",
    "        #print((numerical_labels))\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        #print(outputs.shape)\n",
    "        #print(torch.tensor(numerical_labels).shape)\n",
    "        \n",
    "        loss=criterion(outputs, torch.tensor(numerical_labels).to(device))  # Focal Loss\n",
    "        \n",
    "        \n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update the weights\n",
    "        \n",
    "        #print(f'{run}run done')\n",
    "        #run=run+1\n",
    "        \n",
    "    #print(f'train loss for {epoch+1} epoch = {loss:0.3f} ')\n",
    "    \n",
    "    ###Validation loop:\n",
    "    \n",
    "    model.eval()  # Set to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, text_labels in val_loader:\n",
    "            inputs, text_labels = inputs.to(device), text_labels\n",
    "            \n",
    "            \n",
    "            numerical_labels = [label_to_index[label] for label in text_labels]\n",
    "            numerical_labels=torch.tensor(numerical_labels).to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += len(numerical_labels)\n",
    "            correct += (predicted == numerical_labels).sum().item()\n",
    "            \n",
    "            #step_scheduler.step()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Train Set Loss: {loss.item():.4f}, Validation Set Accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the trained model:\n",
    "torch.save(model.state_dict(), './kaggle/cnn_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default-tensorflow-gpu-2.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "7d6e37093efaa95eb46af3a0496c45ea70f0fe0c24c6fc6929b56d85e591d851"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
