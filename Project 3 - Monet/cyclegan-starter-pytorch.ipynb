{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport cv2\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-22T09:47:14.964694Z","iopub.execute_input":"2024-04-22T09:47:14.965129Z","iopub.status.idle":"2024-04-22T09:47:14.972796Z","shell.execute_reply.started":"2024-04-22T09:47:14.96509Z","shell.execute_reply":"2024-04-22T09:47:14.971753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A PyTorch implementation of the following: https://www.kaggle.com/code/ttymonkey/cyclegan-starter","metadata":{}},{"cell_type":"markdown","source":"# 1. Data Check","metadata":{}},{"cell_type":"code","source":"# Check the root\nroot_path = \"/kaggle/input/gan-getting-started\"\nos.listdir(root_path)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:47:14.974643Z","iopub.execute_input":"2024-04-22T09:47:14.975073Z","iopub.status.idle":"2024-04-22T09:47:14.991825Z","shell.execute_reply.started":"2024-04-22T09:47:14.975034Z","shell.execute_reply":"2024-04-22T09:47:14.99066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading img in rgb\nread_img = lambda path: cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:47:14.99332Z","iopub.execute_input":"2024-04-22T09:47:14.993677Z","iopub.status.idle":"2024-04-22T09:47:14.998785Z","shell.execute_reply.started":"2024-04-22T09:47:14.993647Z","shell.execute_reply":"2024-04-22T09:47:14.998018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load a sample image of monet and photo\ndata_path = f\"{root_path}/photo_jpg\"\nsample_photo = read_img(os.path.join(data_path, os.listdir(data_path)[0]))\n\ndata_path = f\"{root_path}/monet_jpg\"\nsample_monet = read_img(os.path.join(data_path, os.listdir(data_path)[0]))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:47:15.001278Z","iopub.execute_input":"2024-04-22T09:47:15.001892Z","iopub.status.idle":"2024-04-22T09:47:15.02002Z","shell.execute_reply.started":"2024-04-22T09:47:15.001848Z","shell.execute_reply":"2024-04-22T09:47:15.018653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_photo.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:47:15.02199Z","iopub.execute_input":"2024-04-22T09:47:15.022751Z","iopub.status.idle":"2024-04-22T09:47:15.028854Z","shell.execute_reply.started":"2024-04-22T09:47:15.022719Z","shell.execute_reply":"2024-04-22T09:47:15.027637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_photo.min(), sample_photo.max(), sample_photo.dtype","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:47:15.030804Z","iopub.execute_input":"2024-04-22T09:47:15.031527Z","iopub.status.idle":"2024-04-22T09:47:15.045381Z","shell.execute_reply.started":"2024-04-22T09:47:15.031489Z","shell.execute_reply":"2024-04-22T09:47:15.044217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Photo\nplt.subplot(121)\nplt.title(\"Photo\")\nplt.imshow(sample_photo)  \n\n# Monet\nplt.subplot(122)\nplt.title(\"Photo\")\nplt.imshow(sample_monet)  ","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:47:15.047037Z","iopub.execute_input":"2024-04-22T09:47:15.047428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Build The Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.idle":"2024-04-22T09:47:15.582498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# img params for testing\nIMG_H, IMG_W, IMG_C = 256, 256, 3","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:47:15.585002Z","iopub.execute_input":"2024-04-22T09:47:15.585702Z","iopub.status.idle":"2024-04-22T09:47:15.594834Z","shell.execute_reply.started":"2024-04-22T09:47:15.585661Z","shell.execute_reply":"2024-04-22T09:47:15.593534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 Generator","metadata":{}},{"cell_type":"code","source":"def downsample(in_channels, out_channels, kernel_size, norm=True):\n    \"\"\" A simple convolutional block that downsamples the feature map by 2 (stride)\n    e.g. x [256, 256] -> out [128, 128]\n    \n    Same padding is applied\n    \n    The convolutional layer has no bias\n    Group Normalisation is applied is norm=True\n    \n    Flow:\n        x -> Conv2d -> out (x2 smaller) -> GroupNorm -> LeakyRelu\n    \n    \n    \"\"\"\n    downsample_block = [nn.Conv2d(in_channels=in_channels,\n                                  out_channels=out_channels,\n                                  kernel_size=kernel_size,\n                                  stride=2,\n                                  padding=(kernel_size - 1) // 2,\n                                  bias=False)\n                       ]\n    \n    if norm:\n        downsample_block.append(nn.GroupNorm(num_groups=out_channels,\n                                             num_channels=out_channels))\n    \n    \n    downsample_block.append(nn.LeakyReLU())\n    \n    return nn.Sequential(*downsample_block)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:47:15.600092Z","iopub.execute_input":"2024-04-22T09:47:15.600546Z","iopub.status.idle":"2024-04-22T09:47:15.610719Z","shell.execute_reply.started":"2024-04-22T09:47:15.600506Z","shell.execute_reply":"2024-04-22T09:47:15.609439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Check block\nx = torch.randn(1,IMG_C, IMG_H, IMG_W)\n\n# Downsample by 2 and Keep the same number of channels\nout = downsample(3, 3, 3)(x)\n\nassert IMG_C == out.shape[1] \nassert IMG_H == out.shape[2] * 2\nassert IMG_W == out.shape[3] * 2","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:47:15.612134Z","iopub.execute_input":"2024-04-22T09:47:15.612964Z","iopub.status.idle":"2024-04-22T09:47:15.631465Z","shell.execute_reply.started":"2024-04-22T09:47:15.612928Z","shell.execute_reply":"2024-04-22T09:47:15.630244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def upsample(in_channels, out_channels, kernel_size, dropout=True):\n    \"\"\" A simple transpose convolutional block that upsample the feature maps\n    e.g. x [256, 256] -> out [512, 512]\n    \n    Same padding is applied\n    \n    The transpose convolutional layer has no bias\n    Group Normalisation is applied \n    Dropout is applied if drouput=True\n    \n    Flow:\n        x -> Conv2d -> out (x2 bigger) -> GroupNorm -> Dropout -> Relu\n    \n    \n    \"\"\"\n    downsample_block = [nn.ConvTranspose2d(in_channels=in_channels,\n                                           out_channels=out_channels,\n                                           kernel_size=kernel_size,\n                                           stride=2,\n                                           padding=(kernel_size - 1) // 2,\n                                           bias=False),\n                        nn.GroupNorm(num_groups=out_channels,\n                                     num_channels=out_channels)\n                       ]\n    \n    if dropout:\n        downsample_block.append(nn.Dropout(0.5))\n    \n    \n    downsample_block.append(nn.ReLU())\n    \n    return nn.Sequential(*downsample_block)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:47:15.633205Z","iopub.execute_input":"2024-04-22T09:47:15.633591Z","iopub.status.idle":"2024-04-22T09:47:15.645238Z","shell.execute_reply.started":"2024-04-22T09:47:15.633562Z","shell.execute_reply":"2024-04-22T09:47:15.64396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weight_init(m):\n    \"\"\" Initialise Conv2D and ConvTranspose2D with N(0, 0.02)\n    \"\"\"\n    if any(isinstance(m, _m) for _m in [nn.Conv2d, nn.ConvTranspose2d]):\n        nn.init.normal_(m.weight, mean=0.0, std=0.02)\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:47:15.646644Z","iopub.execute_input":"2024-04-22T09:47:15.647018Z","iopub.status.idle":"2024-04-22T09:47:15.661526Z","shell.execute_reply.started":"2024-04-22T09:47:15.646988Z","shell.execute_reply":"2024-04-22T09:47:15.66036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Check block\nx = torch.randn(1,IMG_C, IMG_H, IMG_W)\n\n# Upsample by 2 and Keep the same number of channels\nout = upsample(3, 3, 4)(x)\n\nassert IMG_C == out.shape[1] \nassert IMG_H == out.shape[2] // 2\nassert IMG_W == out.shape[3] // 2","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:47:15.663163Z","iopub.execute_input":"2024-04-22T09:47:15.663512Z","iopub.status.idle":"2024-04-22T09:47:15.691042Z","shell.execute_reply.started":"2024-04-22T09:47:15.663483Z","shell.execute_reply":"2024-04-22T09:47:15.689814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # Encoder and Decoder\n        self.encoder = self._init_encoder()\n        self.decoder = self._init_decoder()\n        \n        # Final Upsample\n        self.out = nn.ConvTranspose2d(in_channels=128,\n                                      out_channels=3,\n                                      kernel_size=4,\n                                      stride=2, padding=1)\n        self.act = nn.Tanh()\n        \n        self.apply(weight_init)\n        \n    def forward(self, x):\n        \"\"\" Downsample and Upsample image\n        \n        \"\"\"\n        \n        # Encode to latent space //128 #512\n        skips = []  # Skip connections U-Net Style\n        for layer in self.encoder:\n            x = layer(x)\n            skips.append(x)\n        \n        # Skip last one, bottom bit, latent space\n        skips = reversed(skips[:-1])\n            \n        \n        # Decode form latent space *64 #128\n        for layer, skip in zip(self.decoder, skips):\n            x = layer(x)\n            x = torch.cat((x, skip), dim=1)\n                \n        # Upsample so that out.shape == x.shape\n        out = self.act(self.out(x))\n        \n        return out\n        \n        \n    def _init_encoder(self):\n        \"\"\" A Sequential Encoder with 7 Downsample Blocks each downsampling by 2\n        \"\"\"\n        base_channels = 64\n        kernel_size = 4\n        \n        encoder = nn.Sequential(\n            # [64, //2, //2]\n            downsample(in_channels=3, out_channels=base_channels, \n                       kernel_size=kernel_size, norm=False), \n            # [128, //4, //4]\n            downsample(in_channels=base_channels, out_channels=base_channels * 2, \n                       kernel_size=kernel_size),\n            \n            # [256, //8, //8]\n            downsample(in_channels=base_channels * 2, out_channels=base_channels * 4, \n                       kernel_size=kernel_size),\n            \n            # [512, //16, //16]\n            downsample(in_channels=base_channels * 4, out_channels=base_channels * 8, \n                       kernel_size=kernel_size),\n            \n            # [512, //32, //32]\n            downsample(in_channels=base_channels * 8, out_channels=base_channels * 8, \n                       kernel_size=kernel_size),\n            \n            # [512, //64, //64]\n            downsample(in_channels=base_channels * 8, out_channels=base_channels * 8, \n                       kernel_size=kernel_size),\n            \n            # [512, //128, //128]\n            downsample(in_channels=base_channels * 8, out_channels=base_channels * 8, \n                       kernel_size=kernel_size),\n            \n        )\n        \n        \n        return encoder\n    \n    def _init_decoder(self):\n        \"\"\" A Sequential Decoder with 6 Upsample Blocks each upling by 2\n        \"\"\"\n        base_channels = 512\n        kernel_size = 4\n        \n        decoder = nn.Sequential(\n            # [512, *2, *2]\n            upsample(in_channels=base_channels, out_channels=base_channels, \n                     kernel_size=kernel_size, dropout=True),\n            \n            # [512, *4, *4]\n            upsample(in_channels=base_channels * 2, out_channels=base_channels, \n                     kernel_size=kernel_size, dropout=True),\n            \n            # [512, *8, *8]\n            upsample(in_channels=base_channels * 2, out_channels=base_channels, \n                     kernel_size=kernel_size, dropout=True),\n            \n            # [256, *16, *16]\n            upsample(in_channels=base_channels * 2, out_channels=base_channels // 2, \n                     kernel_size=kernel_size),\n            \n            # [128, *32, *32]\n            upsample(in_channels=base_channels, out_channels=base_channels // 4, \n                     kernel_size=kernel_size),\n            \n            # [64, *64, *64]\n            upsample(in_channels=base_channels // 2, out_channels=base_channels // 8, \n                     kernel_size=kernel_size),\n            \n        )\n        \n        return decoder","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:47:15.692493Z","iopub.execute_input":"2024-04-22T09:47:15.69283Z","iopub.status.idle":"2024-04-22T09:47:15.712964Z","shell.execute_reply.started":"2024-04-22T09:47:15.692801Z","shell.execute_reply":"2024-04-22T09:47:15.711496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test Generator\ng = Generator()\nx = torch.randn(1,3,256,256)\n\nenc_out = g.encoder(x)\ngen_out = g(x)\n\n# Downsample by 128\nassert enc_out.shape[1] == 512\nassert enc_out.shape[2] == x.shape[2] // 128\nassert enc_out.shape[3] == x.shape[3] // 128\n\n# Upsample to the same shape\nassert gen_out.shape[1] == x.shape[1]\nassert gen_out.shape[2] == x.shape[2]\nassert gen_out.shape[3] == x.shape[3]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:47:15.714248Z","iopub.execute_input":"2024-04-22T09:47:15.714743Z","iopub.status.idle":"2024-04-22T09:47:16.82378Z","shell.execute_reply.started":"2024-04-22T09:47:15.714555Z","shell.execute_reply":"2024-04-22T09:47:16.822951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check weights if initialised correctly \nconvolutions = []\nfor _, m in g.named_modules():\n    if isinstance(m, nn.Conv2d):\n        convolutions.append(m.weight.flatten().clone().detach().numpy())\n        \nconvolutions = np.concatenate(convolutions)\nconvolutions.mean(), convolutions.std()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:47:16.825532Z","iopub.execute_input":"2024-04-22T09:47:16.825974Z","iopub.status.idle":"2024-04-22T09:47:16.925958Z","shell.execute_reply.started":"2024-04-22T09:47:16.825935Z","shell.execute_reply":"2024-04-22T09:47:16.924747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Discriminator","metadata":{}},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        base_channels = 64\n        kernel_size = 4\n        self.discriminator = nn.Sequential(\n            downsample(in_channels=3, out_channels=base_channels, \n                       kernel_size=kernel_size, norm=False),\n            downsample(in_channels=base_channels, out_channels=base_channels * 2, \n                       kernel_size=kernel_size),\n            downsample(in_channels=base_channels * 2, out_channels=base_channels * 4, \n                       kernel_size=kernel_size),\n            nn.ZeroPad2d(padding=(0, 2, 0, 2)),\n            nn.Conv2d(in_channels=base_channels * 4, out_channels=base_channels * 8,\n                      kernel_size=4, stride=1, bias=False),\n            nn.GroupNorm(num_groups=base_channels * 8,\n                         num_channels=base_channels * 8),\n            nn.ZeroPad2d(padding=(0, 2, 0, 2)),      \n            nn.Conv2d(in_channels=base_channels * 8, out_channels=1,\n                      kernel_size=4, stride=1)\n            \n        )\n        \n        self.apply(weight_init)\n        \n    def forward(self, x):\n        \"\"\" Takes an image and produces a reduced feature map \n        \"\"\"\n        \n        return self.discriminator(x)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:47:16.927634Z","iopub.execute_input":"2024-04-22T09:47:16.928087Z","iopub.status.idle":"2024-04-22T09:47:16.939162Z","shell.execute_reply.started":"2024-04-22T09:47:16.928049Z","shell.execute_reply":"2024-04-22T09:47:16.937962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test Discriminator\nd = Discriminator()\nx = torch.randn(1,3,256,256)\n\nassert d(x).shape[1] == 1","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:47:16.94023Z","iopub.execute_input":"2024-04-22T09:47:16.940529Z","iopub.status.idle":"2024-04-22T09:47:17.058422Z","shell.execute_reply.started":"2024-04-22T09:47:16.940504Z","shell.execute_reply":"2024-04-22T09:47:17.057386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 Test with images","metadata":{}},{"cell_type":"code","source":"# To Torch Tensor\ntorch_sample = torch.tensor(sample_photo, dtype=torch.float32)\n\n# -1 to 1\ntorch_sample = (torch_sample / 255.0 - 0.5) / 0.5\n\n# H,W,C to C,H,W\ntorch_sample = torch_sample.permute(2,0,1)\n\n# Add Batch Dim 1,C,H,W\ntorch_sample = torch_sample.unsqueeze(dim=0)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:47:17.059824Z","iopub.execute_input":"2024-04-22T09:47:17.060155Z","iopub.status.idle":"2024-04-22T09:47:17.066792Z","shell.execute_reply.started":"2024-04-22T09:47:17.060129Z","shell.execute_reply":"2024-04-22T09:47:17.065606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monet_generator = Generator()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:47:17.068346Z","iopub.execute_input":"2024-04-22T09:47:17.069085Z","iopub.status.idle":"2024-04-22T09:47:17.852359Z","shell.execute_reply.started":"2024-04-22T09:47:17.069035Z","shell.execute_reply":"2024-04-22T09:47:17.851321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.inference_mode():\n    monet_generator.eval()\n    to_monet = monet_generator(torch_sample).detach()\n    to_monet.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:47:17.853851Z","iopub.execute_input":"2024-04-22T09:47:17.854521Z","iopub.status.idle":"2024-04-22T09:47:18.00777Z","shell.execute_reply.started":"2024-04-22T09:47:17.85448Z","shell.execute_reply":"2024-04-22T09:47:18.00679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(1,2,1)\nplt.title(\"Original Photo\")\nplt.imshow(torch_sample[0].permute(1,2,0) * 0.5 + 0.5)\n\nplt.subplot(1,2,2)\nplt.title(\"Monet Photo\")\nplt.imshow(to_monet[0].permute(1,2,0) * 0.5 + 0.5)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T09:47:18.009296Z","iopub.execute_input":"2024-04-22T09:47:18.009751Z","iopub.status.idle":"2024-04-22T09:47:18.582255Z","shell.execute_reply.started":"2024-04-22T09:47:18.009709Z","shell.execute_reply":"2024-04-22T09:47:18.58109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}